Our opponent model is based on the standard frequency model, but has some tweaks that attempt to improve the quality of the model. The issue weight estimation is still equal to the standard frequency model method. The difference is in the way we approximate values of the items within an issue. The improvement is based on the idea that if the opponent does a new, previously unseen, bid, that this is probably close in utility to the other bids we've seen. This is implemented by changing the so-called learn value addition based on how ``wrong'' our opponent model is for the current bid. \\

In order to estimate how ``wrong'' the estimate is, we first need to decide what offer we expect from the opponent. To do this we make the assumption that new offers the opponent makes are always close to old offers, but usually a bit lower. We also assume that the opponent's utility space has a normal distribution with a mean around $\mu = 0.5$, and a variance around $\sigma^2 = 0.02$. These values have been determined by examining some domains that were present in genius. The expected utility of a bid is calculated when we receive a bid we haven't received before from the opponent. We then look at how many distinct bids we have received ($D$) and what the size of the utility space is (the total number of bids possible, called $T$). We also approximate the average number of bids ignored by the opponent ($I$). We then use the quantile function of the normal distribution to calculate the expected utility:
\begin{align}
  E(U) = \text{normcdf}^{-1} \left(\frac{D \cdot I}{T}\right) = \mu + \sigma \sqrt{2} \cdot \text{erf}^{-1} \left(2 \frac{D \cdot I}{T} - 1\right)
\end{align}

Where $\text{erf}^{-1}(x)$ is the inverse error function. We save this expected utility for each bid and use it again if the bid is made again. Next we check if the utility calculated using the current opponent model is within a certain range of the expected utility. If this is not the case we try a number of different learn value addition values (1 to 100) and check for each of these values what the utility of the current bid would be if we used that value for the frequency modeling algorithm. We pick the learn value addition value that leads to the smallest absolute difference between the utility calculated using our opponent model and the expected utility. \\

The advantage of this method over standard value iteration is that it approximates the opponent model better for agents that are hardheaded. Bids that they have never done before leads to much bigger changes in the opponent model, which leads to a faster convergence towards to true opponent model. However, for agents that concede much faster then the algorithm anticipates (e.g. skip much more that $I$ bids whenever they make a new bid), the opponent model is less accurate. Since we found that most opponents are hardheaded for at least some part of the total time they are active, we thing that, on average, this model will give better results.