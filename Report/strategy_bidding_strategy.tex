The bidding strategy we chose, is not a static strategy, but is time dependent. The strategy is divided in three different phases, with each phase using its own sub-strategy. The main \texttt{Group7\_BS} class decides the current phase by evaluating the normalized time (\texttt{getTime()}) and the two hard-coded phase thresholds, which are corrected for discounts and are stored in the array \texttt{phaseBoundary}. 
The phases itself are all subclasses of the \verb-Phase- class. In this section we discuss the strategy behind each of the phases separately. Note that, because we wanted our acceptance strategy and our opponent model (strategy) to 
work together, we have used a helper class (\verb-Helper-) as the BOA framework did not provide each component with references to all other components. Using the helper class, we can also share values, like the Kalai point, among all our components in order to save computation time and to allow other components to have access to otherwise unavailable information. In case our components are used separately, we ensure that we always have fall back values whenever we request a certain value from the helper class which could be unavailable, since the component that sets that value might be unavailable.

\subsubsection{First Phase}
By offering an opening bid of maximal utility, the opponent can easily estimate our preference profile. Due to this reason we have chosen to offer an initial bid of utility $1.0$ (lower values sometimes lead to bad results where this bid is extremely good for our opponent and is directly accepted).

After determining our opening bid, and receiving the initial bid from the opponent, our agent continues by generating random bids. These random bids are generated within a small utility range (measured by our utility) given by the \verb-getBidRange-. The center of this range decreases by a small amount when time passes.
The center is first set to $1.0$, and ends at $0.9$ (the lower bound). The lower-bound is set to $0.9$, except when our opponent best bid is closer than $0.1$ to $0.9$ utility (in our utility scale). If this is the case the lower bound is set to the utility of this bid of the opponent $+0.1$, and we ensure that the lower-bound and upper-bound do not overlap. The width of the range is set to  $0.02$. 

We do this, because we think that if the opponent can estimate our preference profile better,
we can reach better win-win situations. By conceding this way, we ensure that most opponent 
models accurately profile our preferences.

\subsubsection{Second Phase}

For phase 2, we decided to implement the \emph{Tit-for-Tat} strategy \cite{titfortat}.
We chose this, because it works fairly well against different agents, since you will play
hard against hardheaded opponents, and nicer against conceding agents.
However, as noted in the referenced paper, we should not play too nice,
and try to exploit other opponents. We use the following strategy.

We use the distance in our and the opponent's utility space to the \emph{Kalai-Smorodinsky} point, as a reference to calculate
the amount of concession for both the opponent and our own agent. When conceding, we aim for this point. We chose this point because we think it is the most fair, since for this point our utility is equal to the utility of the opponent. We decided to use the distance to this point instead of only relying on our own utility or opponent estimated utilities, because otherwise we can hardly see whether or not our opponent is conceding. Conceding opponents tend to vary a lot on the scale of our utility,
and utility estimates for the opponent are so noisy, one at least needs to average over some amount of bids. 

We average the change in distance (\verb-getAvgDifferenceKS-) of the last five bids to the Kalai-Smorodinsky point, to determine the mean concession of the opponent. We multiply this by a factor $\sfrac{1}{3}$, and then match their concession by walking towards the KS point. We multiply by this factor to avoid conceding too much in the direction of the opponent. 
We normalize every distance by taking the distance from the Kalai point to the best bid point of our agent and the opponent. We do a linear interpolation between the best bid and the Kalai point (\verb-interpolateBidPoints-).

Furthermore, to ensure our agent does not get stuck when playing against another Tit-for-Tat agent, we will randomly concede from time to time. These concessions are done towards the Kalai-Smorodinsky point, and linearly over $10$ bids, so the opponent can detect our concession more accurately. When the concession has been done, we hope the opponent matches  our concession. If the opponent has done this, we will approach him using Tit-for-Tat, but if not, we take our concession back. 

Finally, we want to reach the most efficient outcomes. Because of this,
we will randomly offer bids on the \emph{Pareto Frontier} $50\%$ of the time (\verb-FindClosestParetoBidPoint()-).
However, to select this bid, we take the Pareto bid closest to our current bid in a special sense.
When determining the closest bid, we weigh our utility twice as much
as the opponent utility (\verb-interpolateBidPoints-), to ensure we don't concede too much by accident 
(this can happen when dealing with strange Pareto Frontiers).

\subsubsection{Third Phase}

The third phase only consists of the very last part of the negotiation session (usually from $t = 0.95$). The strategy that is employed here, offers random bids in a time-dependent decreasing range from $[0.9, 1.0]$ at $t = 0.95$ until $[0.7, 0.8]$ at $t = 1.0$. Also, the approximated opponent model strategy is used, so that we can find out whether we are dealing with a conceding, or HardHeaded opponent. 

When this method returns that the opponent behaves as \emph{conceder}, then our agent maximizes the with-time-decreasing random generated utility $u$ with the the \emph{Kalai-Smodorinsky} point. Otherwise we leave the random generated utility as it is. When this utility $u$ is determined, we choose the best bid within the range $[u-0.01, u+0.01]$. Choosing the best bid is done by fetching all possible bids within this utility range and choosing the one that is best for the opponent by exploiting the information from our opponent model. This of course, increases the probability that the opponent accepts our bid as we are trying to reach the Pareto Frontier. If the opponent model is not available, we can only do as much as choosing a random bid from this range, as nothing about the opponent's preferences is known.

Finding the best bid within the calculated utility range is not guaranteed to succeed, for instance if no bids are available within the given range. Therefore, we have also implemented a fall back mechanism. When determining the next bid has been tried for $15$ times, then a fall back bid is randomly chosen from already offered bids. This way we are sure that a bid is offered without wasting too much time.

