% !TEX root = negotation_final_report.tex
After designing and implementing our agent the BOAconstructor it is time to put it to the test in various situations. In the next two subsections we quantify the performance of our agent. 
We will let our agent compete against various agents from \texttt{ANAC 2011}, we will test the influence of different acceptance strategies for our agent,
we quantify how much our opponent model improves our results, and finally test it on a wide range of domains with and without discounted utilities.
We present the results and discuss the efficiency of the agent in terms of reaching \emph{Nash solutions}, \emph{Kalai Smorodinski points} and outcomes that lie on the \emph{Pareto Optimal Frontier} (POF).

\subsection{Intermediate tests}
During the development of BOAconstructor a lot of tests were run with various versions of the agent. This lead to numerous improvements, both small and big, in our agent. The initial version of the agent barely took the opponent model into account for it's tit for that implementation, but we found that the results of this technique were unsatisfactory, which lead to us implementing the current version of BOAconstructor. 

The initial version of the adaptive opponent model the BOAconstructor used assumed a uniform distribution of the opponent's bids. However, tests showed that the estimated utilities using this model were often incorrect for agents that are very conceding. This lead to the approximation using the inverse normal distribution. Testing also showed that using the estimated utilities from the last time a bid was received lead to better results then just using the most recent result for every bid.

Furthermore, many tests were run to try different parameters for and find bugs in all components. It turned out that bugs kept popping up in all components until very late in the development of the BOAconstructor agent.


\subsection{Performance against \texttt{ANAC 2011} agents}

In this part we compare our agent to three of the agents that attended in the ANAC 2011 competitio: \emph{HardHeaded}, \emph{Gahboninho} and \emph{The Negotiatior}. The results have been obtained by running a tournament using multiple utility profiles for the (undiscounted) Party domain and setting the negotiation time to $30$ seconds,
we use these same settings for further testing also unless otherwise specified. 

\begin{table}[H]
	\centering
	\small
    \begin{tabular}{m{2cm}m{2cm}m{2cm}m{2cm}m{2cm}m{2cm}m{2cm}}
    \toprule
    Agent                       & Average Time Agreement & Average Discounted Util & Average Dist. to Nash & Average Dist. to Pareto & Average Dist. to Kalai \\
    \midrule
    \emph{HardHeaded}		& 0.858  & 0.927  & 0.102  & 0.017  & 0.084   \\ 
    \emph{Gahboninho}   	& 0.887  & 0.918  & 0.032  & 0.001  & 0.035   \\ 
    \emph{The Negotiator} 	& 0.725  & 0.871  & 0.050  & 0.003  & 0.052   \\ 
    \emph{Self}                 & 0.823  & 0.877  & 0.061  & 0.007  & 0.057   \\
    \bottomrule
    \end{tabular}
    \caption{Performance of our agent against \texttt{ANAC 2011} agents (Runtime: $30$s) \label{table:anac2011-results}}
\end{table}

From the results in Table~\ref{table:anac2011-results} we notice we rank third on average utility. 
We notice that HardHeaded and Gahboninho beat us in utility, with a difference of at least $0.04$. The times to agreement for these agents is also higher,
indicating that our agent might accept bids too fast or is conceding too fast in the last phase.
Regarding the effiency of the negotiation we observe that the average distance to the POF, Nash and Kalai upon agreement is very small,
which is a very desirable property for our agent. 

\subsection{Performance of our acceptance strategy}

The next test consists of running our agent with varying \emph{acceptance strategies}. We perform those test so we are able to compare our acceptance strategy with other available ones (our strategy is discussed in Section~\ref{sec:strategyAS}). Again these tests are performed by starting a small tournament. The results are displayed in the Table~\ref{table:as-results}. 

\begin{table}[H]
	\centering
	\small
\begin{tabular}{m{4.0cm}m{3.0cm}m{3.0cm}m{3.0cm}}
  \toprule 
  Agent                         & Average time to agreement & Percentage of agreement & Average utility \\ 
  \midrule
  Our Acceptance Strategy       & 0.777                                                                 & 100 $\%$                                                             & 0.870                                                       \\ 
  Nice-Tit-For-Tat              & 0.867                                                                 & 100 $\%$                                                             & 0.866                                                       \\ 
  Agent K2                      & 0.865                                                                 & 97 $\%$                                                             & 0.865                                                       \\ 
  IAMHaggler2011                & 0.633                                                                 & 97 $\%$                                                             & 0.865                                                       \\ 
  Other - Time \{t=0.99\}       & 0.837                                                                 & 100 $\%$                                                             & 0.855                                                       \\ 
  Other - False                 & 0.851                                                                 & 94 $\%$                                                             & 0.855                                                       \\ 
  Other - Next \{a=1.0. b=0.0\} & 0.873                                                                 & 94 $\%$                                                              & 0.847                                                       \\ 
  Other - Constant \{c=0.9\}    & 0.592                                                                 & 91 $\%$                                                              & 0.814                                                       \\ 
  The Negotiator                & 0.862                                                                 & 100  $\%$                                                            & 0.743                                                       \\ 
  BRAMAgent                     & 0.847                                                                 & 100 $\%$                                                             & 0.729                                                       \\ 
  HardHeaded                    & 0.923                                                                 & 75 $\%$                                                              & 0.680                                                      
  \\
  \bottomrule
\end{tabular}
\caption{Performance of acceptance strategy compared with other acceptance strategies for our agent (Runtime: $30$s) \label{table:as-results}}
\end{table}

We note that combined with our bidding strategy and our opponent model, our acceptance strategy results in the highest average utility.
The nice-tit-for-tat strategy is performing best after our acceptance strategy, just performing slightly worse with $0.004$ difference in average utility.
So, our acceptance strategy is performing better, however the performance is not really significantly higher.
We note our acceptance strategy always reaches an agreement (just as tit-for-tat).
Finally, we note our acceptance strategy is one of the fastest strategy to accept, which indicates our agent might accept bids too fast.

\subsection{Performance of our opponent model}
To test if our opponent model is better than the default hardheaded frequency model, we ran the performance test against the ANAC 2011 agents using the hardheaded frequency model. The results of this are shown in \autoref{table:anac2011-hh}.
\begin{table}[H]
	\centering
	\small
    \begin{tabular}{m{2cm}m{2cm}m{2cm}m{2cm}m{2cm}m{2cm}m{2cm}}
    \toprule
    Agent              & Average Time Agreement & Average Discounted Util & Average Dist. to Nash & Average Dist. to Pareto & Average Dist. to Kalai \\
    \midrule
    \emph{HardHeaded}		& 0.883  & 0.916  & 0.166  & 0.055  & 0.152   \\ 
    \emph{Gahboninho}   	& 0.881  & 0.848  & 0.116  & 0.020  & 0.097   \\ 
    \emph{The Negotiator} 	& 0.756  & 0.868  & 0.092  & 0.027  & 0.079   \\ 
    \emph{Self}                 & 0.839  & 0.806  & 0.125  & 0.035  & 0.140   \\ 
    \bottomrule
    \end{tabular}
    \caption{Performance of our agent with \texttt{HardHeaded Frequency Modeling} (Runtime: $30$s) \label{table:anac2011-hh}}
\end{table}
We see that, compared to our own opponent model, the average utility has decreased by $0.071$, which means that our opponent model leads to  better results for our agents. 
Furthermore, we see that the average utilities for the opponent are also lower compared to what our opponent model achieved.
The distance to the \emph{Nash point}, \emph{Pareto Frontier} and \emph{Kalai point} are especially interesting for evaluating the quality of the opponent model: a better opponent model allows us to offer bids closer to these optimal values. We also note here that for every agent these properties are lowest using our own opponent model.
We conclude our opponent model is significantly better combined with our agent.

\subsection{Performance In Different Domains}
In order to test whether our agent can cope with different domains, we tested it against the same selection of 2011 agents, but using different domains. More specifically, the test was run in the Car/ADG domain, the Amsterdam party domain, the Camera domain, the Nice Or Die domain, the Grocery domain, the IS BT Acquisition  domain, and the Laptop domain. The average results for all agents are shown in \autoref{table:anac2011-domains}.
\begin{table}[H]
  \centering
  \small
  \begin{tabular}{m{2cm}m{2cm}m{2cm}m{2cm}m{2cm}m{2cm}m{2cm}}
  \toprule
    Agent                     & Average Time Agreement & Average Discounted Util & Average Dist. to Nash & Average Dist. to Pareto & Average Dist. to Kalai \\
    \midrule
    \emph{HardHeaded}     & 0.807 & 0.750 & 0.142 & 0.032 & 0.150 \\ 
    \emph{Gahboninho}     & 0.731 & 0.807 & 0.065 & 0.004 & 0.165 \\ 
    \emph{The Negotiator} & 0.694 & 0.646 & 0.288 & 0.000 & 0.220 \\ 
    \emph{Self}           & 0.744 & 0.712 & 0.165 & 0.012 & 0.179 \\ 
    \bottomrule
  \end{tabular}
 \caption{Performance of our agent using a set of diverse domains (Runtime: $30$s) \label{table:anac2011-domains}}
\end{table}
We note that in these domains the average utility of all agents is significantly lower compared to the party domain used in the other tests. We also see that the difference between the utilities is now more significant: the difference between the best and worst average utility is now almost 0.15, whereas this difference was 0.05 for the party domain. 

In order to analyze these results further we decided to create a table in which we compare the average discounted utility in each domain. This allows us to compare the differences per domain. 

The Car domain has a Pareto frontier that allows for high utilities for both agents. As we see in \autoref{table:anac2011-domains2}, most agents seem to exploit this fact well. What is interesting to see is that our agent has the worst average utility for this domain. This is possibly caused by us exploring utilities 0.9 to 1.0 during the initial phase. Enemies will probably not except bids that are worse then what we already offered which leads to us getting a lower utility on average in the car domain, causing a performance that is worse then what we saw in the party domain.

In the Amsterdam Party domain, the average utilities are very close together. The utility space has what looks like a normal distribution for both parties, which means that our agent should be able to construct a reliable opponent model. The bidding strategy can now exploit this knowledge to estimate the Pareto frontier and Kalai Smorodinski point in order to get a good utility for both players and increasing the likeliness that an opponent accepts our offer, leading to us getting a similar or higher average utility to the other top-scoring agents like HardHeaded and Gahboninho. A similar situation in present in the Grocery domain, where it even leads to our agent winning. Note that in contrast with the Amsterdam Party domain, the grocery domain uses a discount factor. A domain like this seems optimal for our agent and gives better results, compared to the other agents, then the party domain does.

The camera domain has a utility space that is a bit skewed towards low utilities. This is what causes the average utilities to be lower. However, since our strategy attempts to move towards the Kalai point, we can exploit spaces where the best option is not close to the optimal option (e.g. not close to utility = 1.0) for both agents, leading to a higher average utility, and in this case even leading to BOAconstructor getting the best average utility. 

The nice or die domain has only 3 possible bids: if both players concede, they get a utility of 0.3, but if only one of them concedes the utility is good for the other player, but bad for the first one. We see that The Negotiator always concedes to what is best for the other players, while HardHeaded and Gahboninho are more conservative. Since our model accepts the opponent's bid if the time has almost run out, we have a lower average utility here. Because the domain is so small the differences in average utility are very big, and it's hard to compare these results to the party domain.

The LS BT Acquisition domain is concentrated at the higher utilities: all possible bids have a utility of $0.65$ or higher for both players. This probably caused our opponent model to be inaccurate, which in turn leads to a wrong approximation the Kalai-point. We think this caused us to accept offers that were in fact not very good, and thus to us getting the lowest average utility, by far, from all agents.

The laptop domain is very small; it has only two bids on the Pareto frontier. The domain is laid out in such a way that conceding almost immediately leads to unfortunate bids, which makes it hard to get to the optimal agreement. However the points are nicely distributed over the domain, so the agent can create a reliable opponent model, and thus focus on Pareto optimal bids. Thus, our agent still performs reasonable well, in line with what we expect from our tests in the party domain.


\begin{table}[H]
  \centering
  \small
  \begin{tabular}{lp{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}}
  \toprule
  Domain                    & HardHeaded     & Gahboninho     & The Negotiator & Self \\ 
  \midrule
  Car                       & \textbf{0.972} & 0.923          & 0.960          & 0.893 \\
  Amsterdam Party           & \textbf{0.850} & 0.843          & 0.808          & 0.834 \\ 
  Camera                    & 0.766          & 0.698          & 0.732          & \textbf{0.777} \\ 
  Nice or Die               & 0.500          & \textbf{1.000} & 0.160          & 0.413 \\ 
  Grocery                   & 0.689          & 0.705          & 0.572          & \textbf{0.758} \\ 
  IS BT Acquisition         & 0.762          & \textbf{0.809} & 0.803          & 0.683 \\ 
  Laptop                    & \textbf{0.711} & 0.673          & 0.489          & 0.627 \\ 
  \bottomrule
  \end{tabular}
  \caption{Average discounted utilities of the tested agents in the given domains (Runtime: $30$s) \label{table:anac2011-domains2}}
\end{table}

Overall we see that our agent generally performs well in domains with discounted utilities, sometimes even better than in domains without discounted utilities. However in the IS BT Acquisition domain, the performance of our agent is suboptimal, and this might be influenced by the fact that our agent does not take the discount factor into account. To confirm this, we ran the same tests again, but we changed the agent so that the length of the phases in the bidding strategy were adjusted automatically. This should lead to faster agreement when the discount factor is high. The results of this are shown in \autoref{table:anac2011-domains3}. It is clear from these results that the average utilities are very similar, or worse, for this discount adjusted agent compared to the original agent.

\begin{table}[H]
  \centering
  \small
  \begin{tabular}{lp{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}}
      \toprule
    Domain & HardHeaded & Gahboninho & The Negotiator & Self \\
    \midrule
    Camera & 0.785 & 0.703 & \textbf{0.773} & 0.765 \\
    Grocery & 0.724 & 0.705 & \textbf{0.827} & 0.733 \\
    IS BT Acquisition BT prof & 0.768 & \textbf{0.812} & 0.810 & 0.689 \\
    Laptop & \textbf{0.713} & 0.670 & 0.600 & 0.673 \\
   \bottomrule
  \end{tabular}
  \caption{Average discounted utilities of the tested agents in the given domains with phases scaled to bidding discount (Runtime: $30$s) \label{table:anac2011-domains3}}
\end{table}
